{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 22: stochastic gradient descent\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=lmqV5Z5HZzc&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 夜に駆ける by YOASOBI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 solution posted\n",
    "- a5 posted\n",
    "- Final exam scheduling survey: https://ubc.ca1.qualtrics.com/jfe/form/SV_0cGkvmKdr8Aq6SG\n",
    "- Bug fixed from last class, see https://edstem.org/us/courses/3226/discussion/306056\n",
    "- Tutorial was missed this morning, rescheduled for 5pm today (right after class)\n",
    "  - See Canavas for Zoom link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- kernel trick recap\n",
    "- stochastic gradient motivation\n",
    "- SGD vs. GD\n",
    "- minimizing averages\n",
    "- per-example gradients\n",
    "- visualizing SGD: 1 parameter\n",
    "- decreasing step sizes\n",
    "- stochastic average gradient\n",
    "- visualizing SGD: 2 parameters\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap the different decisions you make:\n",
    "\n",
    "1. Pick a model <-- affects overfitting \n",
    "2. Pick a loss <-- affects overfitting (e.g. regularization)\n",
    "3. Pick an optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. For stochastic gradient to work, we need to increase the step size as the optimization proceeds.\n",
    "2. Stochastic gradient can be used for training many models, including linear regression and logistic regression.\n",
    "3. An iteration of stochastic gradient might cause the loss might go up, even for a very small learning rate.\n",
    "4. It is reasonable to check whether the loss went up or down after each iteration of stochastic gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. False; it should say _decrease_ rather than _increase_. \n",
    "2. True\n",
    "3. True; note that \"the loss\" refers to the overall loss, which is a sum over training examples\n",
    "4. False; the point of SGD is to not do an $O(n)$ operation at every iteration, but computing the loss is an $O(n)$ operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk about minibatches, iterations, and epochs\n",
    "\n",
    "- SGD as defined in the video looks at 1 example at a time\n",
    "- But you could look at more than one example at a time\n",
    "- The number of examples you look at (per iteration) is called the _minibatch size_ or _batch size_\n",
    "- An _iteration_ is one update to $w$\n",
    "  - In regular gradient descent, you look at $n$ examples per iteration\n",
    "  - In SGD, you look at $k$ examples per iteration, where $k$ is the minibatch size\n",
    "- An _epoch_ is the number of iterations it takes to pass through the whole training set.\n",
    "  - If minibatch size is $1$, then you have $n$ iterations per epoch\n",
    "  - If minibatch size is $10$, then you have $n/10$ iterations per epoch\n",
    "- Why am I bothering you with this?\n",
    "  - If you tell me you did 1000 iterations of SGD, I don't know if that's a lot or a little; it really depends on your minibatch size.\n",
    "  - If you tell me you did 1000 epochs of SGD, then I have an idea that this is quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More T/F questions\n",
    "\n",
    "These questions pertain to the idea of batch size (aka minibatch size) and epochs:\n",
    "\n",
    "1. Increasing the batch size results in slower, but better, iterations.\n",
    "2. If you double the number of epochs and double the batch size, then nothing has changed.\n",
    "3. Increasing the batch size makes one epoch slower.\n",
    "4. One epoch of stochastic gradient takes about the same amount of time as one iteration of gradient descent.\n",
    "5. Stochastic gradient with a minibatch size of $n$ is the same thing as gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. True. Especially if you say \"typically better\". \n",
    "2. False. _The number of iterations has not changed_. The total amount of computation has changed (in fact doubled). \n",
    "3. This one is a mess. False-ish. An epoch represents a certain amount of computation, so changing the batch size should not affect the speed. That argument so far is within the scope of the course. Here is some more info beyond the scope of the course: The reality depends on hardware and other considerations. If you have parallel processing available then it might actually be faster with a bigger batch size because you can do the whole batch at once in parallel. \n",
    "4. True-ish (but see above note on parallel processing).\n",
    "5. True if you're sampling without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n",
    "\n",
    "#### 2016W2 final\n",
    "\n",
    "Name an advantage and a disadvantage of using a smaller minibatch size in stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018W1 final\n",
    "\n",
    "![](img/L22/parta.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/L22/partb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is also a part (c), but it's harder than what I would ask about SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
