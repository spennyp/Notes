{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 30: neural networks: `fit`\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=3l9pyhpxGtU&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n",
    "1. 說好不哭 by Jay Chou\n",
    "2. Body Gold by Oh Wonder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- Mike's office hours rescheduled this week, to tomorrow at 9:30am\n",
    "- Final exam survey results: https://edstem.org/us/courses/3226/discussion/353224 looking to hear some rationales.\n",
    "  - Option 1: time limit\n",
    "  - Option 2: no time limit, but you can't go back after answering a question\n",
    "- classes left (including today): 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- neural networks recap\n",
    "- optimization\n",
    "- backpropagation\n",
    "- classification\n",
    "- ImageNet \n",
    "- training with SGD\n",
    "- parameter initialization\n",
    "- SGD step size\n",
    "- vanishing gradients & ReLUs\n",
    "- regularization\n",
    "- early stopping\n",
    "- dropout\n",
    "- vocabulary\n",
    "- convolutions (setting the stage for the next lecture)\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes since 2018:\n",
    "\n",
    "- dropout less relevant in 2021\n",
    "- new activation function: swish (similar to a ReLU but not monotonic and not flat for half of it)\n",
    "\n",
    "Some comments on classification:\n",
    "\n",
    "- we put a softmax at the end\n",
    "- other nonlinearities are elementwise, but softmax isn't\n",
    "- softmax loss also called cross-entropy loss in deep learning literature\n",
    "\n",
    "Movie that came out on Netflix a couple days ago: Coded Bias\n",
    "\n",
    "Note to self: update name of NeurIPS on webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016W2 final\n",
    "\n",
    "Why do we need to carefully intialize our parameters with deep learning, whereas we didn’t pay much attention to this issue for logistic regression and SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** our loss in non convex, if we choose a bad initialization we may end up with a bad result. Example: initializing all weights as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016W1\n",
    "\n",
    "(Several typos fixed from the original)\n",
    "\n",
    "Consider a two-layer neural network model with L2-regularization,\n",
    "\n",
    "$$f\\left(W^{(1)},W^{(2)},w\\right)=\\frac12 \\sum_{i=1}^n \\left(w^Th(W^{(2)}h(W^{(1)}x_i))-y_i\\right)^2\\\\ +\\frac{\\lambda}{2}\\|w\\|^2+\\frac{\\lambda_1}{2}\\|W^{(1)}\\|^2_F+\\frac{\\lambda_2}{2}\\|W^{(2)}\\|^2_F$$\n",
    "\n",
    "where $W^{(1)}$ is $k_1\\times d$ and $W^{(2)}$ is $k_2\\times k_1$ and $w$ is $k_2 \\times 1$. \n",
    "\n",
    "Describe how the following factors affect the two parts of the fundamental tradeoff:\n",
    "\n",
    "1. $k_1$\n",
    "2. $\\lambda$\n",
    "3. Increasing the depth (number of hidden layers) from 2 to a larger number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. Increasing $k_1$ creates a more complex model because it has more parameters. Therefore increasing $k_1$ means lower training error, higher approximation error.\n",
    "2. $\\lambda$ is a regularization hyperparameter means to reduce overfitting. Increasing $\\lambda$ increases training error and decreases approximation error.\n",
    "3. Again as with $k_1$, increasing this gives us more parameters, thus more complexity, thus lower train error and higher approx error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018W1 final\n",
    "\n",
    "How many trainable parameters does a fully-connected neural network for $k$-class classification have, assuming one hidden layer of size $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** You have layer sizes $(d,m,k)$\n",
    "\n",
    "- Weights: $dm+mk$\n",
    "- Biases: $m+k$\n",
    "- Total: $dm+mk+m+k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also 2018W1 final\n",
    "\n",
    "![](img/L30/convex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** \n",
    "\n",
    "- ii, iii, iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/341830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_venn2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
